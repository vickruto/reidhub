{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd93 Welcome to ReIDHub Docs","text":"<p>Welcome to the documentation for ReIDHub \u2014 a central hub for open-source animal re-identification (ReID) datasets, built on the Access\u2013Assess\u2013Address framework.</p>"},{"location":"#about-reidhub","title":"\ud83c\udf0d About ReIDHub","text":"<p>Animal re-identification is a critical task in wildlife monitoring and conservation, yet workflows for accessing, processing, and analyzing open-source datasets are often fragmented. ReIDHub addresses this challenge by providing a structured, extensible framework that makes dataset-driven research in animal ReID portable, reproducible, and collaborative.</p> <p>The framework is built around three pillars:</p> <ol> <li>Access \u2014 Retrieve open-source ReID datasets in standardized formats and explore them with tools like FiftyOne.  </li> <li>Assess \u2014 Compute dataset statistics, detect potential image quality issues (e.g., overexposure, motion blur), and generate reusable artifacts such as embeddings and open-source model predictions \u2014 all with caching.  </li> <li>Address \u2014 Tackle scientific and practical questions by benchmarking datasets with models like <code>MegaDescriptor</code>, or classical approaches such as <code>SIFT</code>.</li> </ol> <p>Beyond datasets, ReIDHub also centralizes other research outputs related to the datasets  \u2014 including original citations, related publications, and enriched outputs \u2014 making it easier to build upon previous work.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>New to ReIDHub? Start here:  </p> <ul> <li>\ud83d\udce5 Installation \u2014 step-by-step setup guide  </li> <li>\u26a1 Quick Start \u2014 run your first workflow in minutes  </li> <li>\u2753 FAQ \u2014 answers to common questions  </li> </ul>"},{"location":"#supported-datasets","title":"\ud83d\udcda Supported Datasets","text":"<p>ReIDHub provides standardized access and enrichments for a growing catalogue of datasets, including:  </p> <ul> <li>Great Zebra and Giraffe Count </li> <li>Nyala Data (upcoming) </li> <li>Lions Data (upcoming) </li> <li>\u2026and more  </li> </ul> <p>Each dataset page includes high-level information, sample images, access instructions, standardized metadata, enrichments, usage notes, and citations.  </p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>Standardized Dataset Access \u2014 work with diverse ReID datasets through a consistent interface.  </li> <li>Dataset Mirroring \u2014 redundancy via open repositories (e.g., Hugging Face, Kaggle).  </li> <li>Data Enrichment \u2014 reusable artifacts such as embeddings, bounding box predictions, and quality checks.  </li> </ul>"},{"location":"#navigation","title":"\ud83e\udded Navigation","text":"<p>Use the sidebar to explore:  </p> <ul> <li>\ud83d\udcd6 Tutorials \u2014 practical, hands-on guides  </li> <li>\u2699\ufe0f API Reference \u2014 developer-focused documentation  </li> <li>\ud83e\udd1d Contributing \u2014 how to get involved  </li> <li>\ud83d\udcdd Changelog \u2014 see what\u2019s new  </li> </ul> <p>\ud83d\udca1 Tip: If you\u2019re here for the first time, jump straight to Quick Start and get your first ReID workflow running in minutes.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#access","title":"Access","text":""},{"location":"api/#great-zebra-and-giraffe-count","title":"Great Zebra and Giraffe Count","text":"<p>Examples:</p> <pre><code>from reidhub.access.provenance.gzgc import download_and_extract\ndownload_and_extract()\n</code></pre>"},{"location":"api/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/#dataset-parsing","title":"Dataset Parsing","text":""},{"location":"api/#load-all-datasets-yaml-config","title":"Load All Datasets YAML Config","text":""},{"location":"api/#reidhub.access.provenance.dataset_parser.load_datasets","title":"reidhub.access.provenance.dataset_parser.load_datasets","text":"<pre><code>load_datasets(file_path: str) -&gt; Dict[str, DatasetConfig]\n</code></pre> <p>Loads datasets from a YAML file and returns them as DatasetConfig objects.</p> Source code in <code>reidhub/access/provenance/dataset_parser.py</code> <pre><code>def load_datasets(file_path: str) -&gt; Dict[str, DatasetConfig]:\n    \"\"\"\n    Loads datasets from a YAML file and returns them as DatasetConfig objects.\n    \"\"\"\n    with open(file_path, \"r\") as file:\n        data = yaml.safe_load(file)\n\n    datasets = {}\n    for dataset_name, dataset_data in data.items():\n        datasets[dataset_name] = DatasetConfig(**dataset_data)\n\n    return datasets\n</code></pre>"},{"location":"api/#load-single-dataset-yaml-config","title":"Load Single Dataset YAML Config","text":""},{"location":"api/#reidhub.access.provenance.dataset_parser.get_dataset_config","title":"reidhub.access.provenance.dataset_parser.get_dataset_config","text":"<pre><code>get_dataset_config(DATASET_ID: str) -&gt; DatasetConfig\n</code></pre> <p>Loads the dataset configuration for a given dataset name.</p> Source code in <code>reidhub/access/provenance/dataset_parser.py</code> <pre><code>def get_dataset_config(DATASET_ID: str) -&gt; DatasetConfig:\n    \"\"\"\n    Loads the dataset configuration for a given dataset name.\n    \"\"\"\n    with open(DATASET_YAML_PATH, \"r\") as file:\n        datasets = yaml.safe_load(file)\n\n    dataset_data = datasets.get(DATASET_ID)\n    dataset_data = DatasetConfig(**dataset_data)\n\n    return dataset_data  # add a None return option\n</code></pre>"},{"location":"api/access/","title":"Reference for <code>reidhub/access</code>","text":""},{"location":"api/access/#a-provenance","title":"a. Provenance","text":"<p>Provenance (noun) 1. The place of origin or earliest known history of something. 2. (informal) The origin story of a dataset. </p> <p>This sub module provides functionality for accessing raw datasets for the first from the original source.  Its functionalities include <code>downloading</code>, <code>systematizing</code> and <code>cataloguing</code> a dataset from its raw form.</p>"},{"location":"api/access/#great-zebra-and-giraffe-count","title":"Great Zebra and Giraffe Count","text":"<p>Examples:</p> <pre><code>from reidhub.access.provenance.gzgc import download_and_extract\ndownload_and_extract()\n</code></pre>"},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/access/#nyala-data","title":"Nyala Data","text":"<p>Examples:</p> <pre><code>from reidhub.access.provenance.nyala import download_and_extract\ndownload_and_extract()\n</code></pre>"},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/access/#sea-star-re-id-2023","title":"Sea Star Re-ID 2023","text":""},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/access/#b-enriched","title":"b. Enriched","text":"<p>Enriched (adjective) 1. Made fuller, more valuable, or more informative. 2. (informal) The return journey of a once-accessed and catalogued dataset</p> <p>This sub module provides functionality for re-accessing the <code>processed</code>, <code>systematized</code> and <code>enriched</code> versions of the catalogued datasets from online mirrors including <code>Hugging Face</code> and <code>Kaggle</code>. </p>"},{"location":"api/access/#great-zebra-and-giraffe-count_1","title":"Great Zebra and Giraffe Count","text":"<p>Examples:</p> <pre><code>from reidhub.access.enriched.gzgc import load\nload()\n</code></pre>"},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/access/#nyala-data_1","title":"Nyala Data","text":"<p>Examples:</p> <pre><code>from reidhub.access.enriched.nyala import load\nload()\n</code></pre>"},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/access/#sea-star-re-id-2023_1","title":"Sea Star Re-ID 2023","text":"<p>Examples:</p> <pre><code>from reidhub.access.enriched.seastars import load\nload()\n</code></pre>"},{"location":"api/access/#reidhub.access.provenance.gzgc.download_and_extract","title":"reidhub.access.provenance.gzgc.download_and_extract","text":"<pre><code>download_and_extract() -&gt; str\n</code></pre> <p>downloads the gzgc from the gcp bucket url provided by lila datasets. Accessible here: lila gzgc</p> <p>Args:</p> <p>Returns:</p> Type Description <code>str</code> <p>a path to the extracted and formatted <code>reidhub</code> dataset</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n&gt;&gt;&gt; download_and_extract()\n</code></pre> Source code in <code>reidhub/access/provenance/gzgc.py</code> <pre><code>def download_and_extract() -&gt; str:\n    \"\"\"\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: [lila gzgc](https://lila.science/datasets/great-zebra-giraffe-id)\n    &lt;!-- DATASET_ID (str) :- the identifier for the dataset. --&gt;\n\n    Args:\n\n    Returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n    Examples:\n        &gt;&gt;&gt; from reidhub.provenance.gzgc import download_and_extract\n        &gt;&gt;&gt; download_and_extract()\n\n    \"\"\"\n    config = get_dataset_config(DATASET_ID)\n    url = config.url[0]\n    filename = url.split(\"/\")[-1]\n\n    # Create dataset-specific cache directory\n    dataset_dir = Path(defaults[\"cache_root\"]) / DATASET_ID\n    dataset_dir = dataset_dir.expanduser().resolve()\n    tar_path = dataset_dir / filename\n    dataset_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download the file if not already present\n    if not tar_path.exists():\n        logging.info(f\"Downloading {filename} to {tar_path}...\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status()\n            with open(tar_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            logging.info(f\"Download complete: {tar_path}\")\n        except requests.exceptions.RequestException as e:\n            logging.error(f\"Error downloading {filename}: {e}\")\n            return str(dataset_dir)\n    else:\n        logging.info(f\"{tar_path} already exists. Skipping download.\")\n\n    # Extract the file into the dataset directory\n    extracted_flag = dataset_dir / \".extracted\"\n    if not extracted_flag.exists():\n        logging.info(f\"Extracting {tar_path} into {dataset_dir}...\")\n        try:\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=dataset_dir)\n            extracted_flag.touch()  # mark extraction complete\n            logging.info(\"Extraction complete.\")\n        except tarfile.TarError as e:\n            logging.error(f\"Error extracting {tar_path}: {e}\")\n    else:\n        logging.info(f\"Extraction already completed for {tar_path}.\")\n\n    return str(dataset_dir)\n</code></pre>"},{"location":"api/address/","title":"Reference for <code>reidhub/address</code>","text":""},{"location":"api/assess/","title":"Reference for <code>reidhub/assess</code>","text":""},{"location":"api/assess/#a-statics","title":"a. Statics","text":""},{"location":"api/assess/#reidhub.assess.statics.plot_grid","title":"reidhub.assess.statics.plot_grid","text":"<pre><code>plot_grid(\n    images: List[Union[ndarray, Image]],\n    ids: List[int],\n    grid_shape: Tuple[int, int] = (3, 3),\n    img_size: Tuple[int, int] = (224, 224),\n    spacing: float = 0.05,\n) -&gt; plt.Figure\n</code></pre> <p>Plot a grid of images with colored borders per identity.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[Union[ndarray, Image]]</code> <p>List of images (either numpy arrays or PIL images).</p> required <code>ids</code> <code>List[int]</code> <p>List of identity labels corresponding to each image.</p> required <code>grid_shape</code> <code>Tuple[int, int]</code> <p>Shape of the grid as (rows, cols). Default is (3, 3).</p> <code>(3, 3)</code> <code>img_size</code> <code>Tuple[int, int]</code> <p>The (height, width) to resize the images. Default is (224, 224).</p> <code>(224, 224)</code> <code>spacing</code> <code>float</code> <p>Fractional spacing between subplots. Default is 0.05.</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The figure containing the grid of images with borders.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; images = [np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8) for _ in range(9)]\n&gt;&gt;&gt; ids = [1, 1, 2, 2, 3, 3, 4, 4, 5]\n&gt;&gt;&gt; fig = plot_grid(images, ids, grid_shape=(3, 3), img_size=(100, 100), spacing=0.1)\n&gt;&gt;&gt; plt.close(fig)  # Close the figure to prevent display in non-interactive environments\n</code></pre> Source code in <code>reidhub/assess/statics.py</code> <pre><code>def plot_grid(\n    images: List[Union[np.ndarray, Image.Image]],\n    ids: List[int],\n    grid_shape: Tuple[int, int] = (3, 3),\n    img_size: Tuple[int, int] = (224, 224),\n    spacing: float = 0.05,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot a grid of images with colored borders per identity.\n\n    Args:\n        images (List[Union[np.ndarray, PIL.Image.Image]]): List of images (either numpy arrays or PIL images).\n        ids (List[int]): List of identity labels corresponding to each image.\n        grid_shape (Tuple[int, int], optional): Shape of the grid as (rows, cols). Default is (3, 3).\n        img_size (Tuple[int, int], optional): The (height, width) to resize the images. Default is (224, 224).\n        spacing (float, optional): Fractional spacing between subplots. Default is 0.05.\n\n    Returns:\n        (matplotlib.figure.Figure): The figure containing the grid of images with borders.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; images = [np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8) for _ in range(9)]\n        &gt;&gt;&gt; ids = [1, 1, 2, 2, 3, 3, 4, 4, 5]\n        &gt;&gt;&gt; fig = plot_grid(images, ids, grid_shape=(3, 3), img_size=(100, 100), spacing=0.1)\n        &gt;&gt;&gt; plt.close(fig)  # Close the figure to prevent display in non-interactive environments\n    \"\"\"\n    # Unpack grid shape\n    cols, rows = grid_shape\n\n    # Calculate total number of cells in the grid\n    n = rows * cols\n\n    # Sample n images if more are provided\n    if len(images) &gt; n:\n        idxs = np.random.choice(len(images), n, replace=False)\n        images = [images[i] for i in idxs]\n        ids = [ids[i] for i in idxs]\n\n    # Normalize identities into a color map\n    unique_ids = sorted(set(ids))\n    cmap = plt.cm.get_cmap(\"tab20\", len(unique_ids))  # Updated cmap access\n    id2color = {uid: cmap(i) for i, uid in enumerate(unique_ids)}\n\n    # Create subplots\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))\n    axes = np.array(axes).reshape(rows, cols)\n\n    # Set transparent background\n    fig.patch.set_alpha(0)\n\n    # Plot each image with its border\n    for ax, img, identity in zip(axes.flatten(), images, ids):\n        # Convert to PIL Image if necessary and resize\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_resized = img.resize(img_size)\n\n        # Display image\n        ax.imshow(img_resized)\n        ax.axis(\"off\")\n\n        # Draw border around the image\n        rect = patches.Rectangle(\n            (0, 0),\n            img_size[0],\n            img_size[1],\n            linewidth=10,\n            edgecolor=id2color[identity],\n            facecolor=\"none\",\n            transform=ax.transData,\n        )\n        ax.add_patch(rect)\n\n    # Remove extra axes if fewer images are provided\n    for ax in axes.flatten()[len(images) :]:\n        ax.axis(\"off\")\n\n    # Adjust spacing between subplots\n    plt.subplots_adjust(wspace=spacing, hspace=spacing)\n\n    return fig\n</code></pre>"},{"location":"api/assess/#reidhub.assess.statics.plot_identity_histogram","title":"reidhub.assess.statics.plot_identity_histogram","text":"<pre><code>plot_identity_histogram(\n    ids: List[Union[int, str]],\n    bins: Union[int, Sequence[float]] = 50,\n    log_scale: bool = False,\n    alpha: float = 0.6,\n    figsize: Tuple[float, float] = (8, 5),\n) -&gt; plt.Figure\n</code></pre> <p>Plot a transparent histogram of identity frequencies (how many images per identity).</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[Union[int, str]]</code> <p>List of identity labels (e.g., integers or strings).</p> required <code>bins</code> <code>Union[int, Sequence[float]]</code> <p>Number of bins or explicit bin edges for the histogram. Defaults to 50.</p> <code>50</code> <code>log_scale</code> <code>bool</code> <p>Whether to use a logarithmic scale for the y-axis. Defaults to False.</p> <code>False</code> <code>alpha</code> <code>float</code> <p>Transparency of histogram bars (0=fully transparent, 1=opaque). Defaults to 0.6.</p> <code>0.6</code> <code>figsize</code> <code>Tuple[float, float]</code> <p>Figure size as (width, height) in inches. Defaults to (8, 5).</p> <code>(8, 5)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.pyplot.Figure: The generated histogram figure.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ids = [\"zebra1\", \"zebra1\", \"zebra2\", \"zebra3\", \"zebra3\", \"zebra3\"]\n&gt;&gt;&gt; fig = plot_identity_histogram(ids, bins=3, log_scale=False, alpha=0.7, figsize=(6, 4))\n&gt;&gt;&gt; plt.close(fig)  # Close the figure to prevent display in non-interactive environments\n</code></pre> Source code in <code>reidhub/assess/statics.py</code> <pre><code>def plot_identity_histogram(\n    ids: List[Union[int, str]],\n    bins: Union[int, Sequence[float]] = 50,\n    log_scale: bool = False,\n    alpha: float = 0.6,\n    figsize: Tuple[float, float] = (8, 5),\n) -&gt; plt.Figure:\n    \"\"\"Plot a transparent histogram of identity frequencies (how many images per identity).\n\n    Args:\n        ids: List of identity labels (e.g., integers or strings).\n        bins: Number of bins or explicit bin edges for the histogram. Defaults to 50.\n        log_scale: Whether to use a logarithmic scale for the y-axis. Defaults to False.\n        alpha: Transparency of histogram bars (0=fully transparent, 1=opaque). Defaults to 0.6.\n        figsize: Figure size as (width, height) in inches. Defaults to (8, 5).\n\n    Returns:\n        matplotlib.pyplot.Figure: The generated histogram figure.\n\n    Examples:\n        &gt;&gt;&gt; ids = [\"zebra1\", \"zebra1\", \"zebra2\", \"zebra3\", \"zebra3\", \"zebra3\"]\n        &gt;&gt;&gt; fig = plot_identity_histogram(ids, bins=3, log_scale=False, alpha=0.7, figsize=(6, 4))\n        &gt;&gt;&gt; plt.close(fig)  # Close the figure to prevent display in non-interactive environments\n    \"\"\"\n    # Count how many images per identity\n    counts = list(Counter(ids).values())\n\n    # Plot histogram\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.hist(counts, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=alpha)\n\n    # Transparent backgrounds\n    fig.patch.set_alpha(0)  # Figure background\n    ax.patch.set_alpha(0)  # Axes background\n\n    ax.set_xlabel(\"Number of images per identity\")\n    ax.set_ylabel(\"Number of identities\")\n    ax.set_title(\"Identity Frequency Distribution\")\n\n    if log_scale:\n        ax.set_yscale(\"log\")\n        ax.set_ylabel(\"Number of identities (log scale)\")\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/assess/#b-enrichment","title":"b. Enrichment","text":""},{"location":"api/assess/#c-utils","title":"c. Utils","text":""},{"location":"api/assess/#reidhub.assess.utils.get_number_of_images","title":"reidhub.assess.utils.get_number_of_images","text":"<pre><code>get_number_of_images(dataset_root: str) -&gt; int\n</code></pre> <p>Get the number of images in a dataset from its root path</p> <p>Parameters:</p> Name Type Description Default <code>dataset_root</code> <code>str</code> <p>The root path of the dataset</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of images in the directory</p> Source code in <code>reidhub/assess/utils.py</code> <pre><code>def get_number_of_images(dataset_root: str) -&gt; int:\n    \"\"\"\n    Get the number of images in a dataset from its root path\n\n    Args:\n        dataset_root (str): The root path of the dataset\n\n    Returns:\n        int: Number of images in the directory\n    \"\"\"\n    image_paths = find_images(dataset_root)\n    return len(image_paths)\n</code></pre>"},{"location":"api/assess/#reidhub.assess.utils.get_directory_size","title":"reidhub.assess.utils.get_directory_size","text":"<pre><code>get_directory_size(dataset_root: Union[str, Path]) -&gt; int\n</code></pre> <p>Calculate the total size of a directory and its subdirectories in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_root</code> <code>str</code> <p>The root path of the dataset</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total size in bytes, or 0 if directory doesn't exist</p> Source code in <code>reidhub/assess/utils.py</code> <pre><code>def get_directory_size(dataset_root: Union[str, Path]) -&gt; int:\n    \"\"\"\n    Calculate the total size of a directory and its subdirectories in bytes.\n\n    Args:\n        dataset_root (str): The root path of the dataset\n\n    Returns:\n        int: Total size in bytes, or 0 if directory doesn't exist\n    \"\"\"\n    total_size = 0\n\n    try:\n        # Walk through directory and subdirectories\n        for dirpath, _, filenames in os.walk(dataset_root):\n            # Add size of each file\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except (OSError, FileNotFoundError):\n                    # Skip files that can't be accessed\n                    continue\n\n        return total_size\n\n    except (OSError, FileNotFoundError):\n        # Return 0 if directory doesn't exist or can't be accessed\n        return 0\n</code></pre>"},{"location":"api/assess/#reidhub.assess.utils.check_for_duplicates","title":"reidhub.assess.utils.check_for_duplicates","text":"<pre><code>check_for_duplicates(dataset: Dataset) -&gt; fo.Dataset\n</code></pre> <p>Compute uniqueness of dataset samples and check for duplicates</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>the dataset to check for duplicates.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>fo.Dataset: the dataset with the <code>uniqueness</code> and <code>duplicates</code> fields added</p> Source code in <code>reidhub/assess/utils.py</code> <pre><code>def check_for_duplicates(dataset: fo.Dataset) -&gt; fo.Dataset:\n    \"\"\"\n    Compute uniqueness of dataset samples and check for duplicates\n\n    Args:\n        dataset (fo.Dataset): the dataset to check for duplicates.\n\n    Returns:\n        fo.Dataset: the dataset with the `uniqueness` and `duplicates` fields added\n\n    \"\"\"\n    fob.compute_uniqueness(dataset)\n\n    # check for near duplicates\n    index = fob.compute_near_duplicates(dataset)\n    dups_view = index.duplicates_view()\n    dups_view.tag_samples(\"duplicate\")\n\n    return dataset\n</code></pre>"},{"location":"api/assess/#reidhub.assess.utils.fiftyone_check_image_quality_issues","title":"reidhub.assess.utils.fiftyone_check_image_quality_issues  <code>async</code>","text":"<pre><code>fiftyone_check_image_quality_issues(\n    dataset: Dataset, operations: List[str] = IMAGE_ISSUES_OPERATIONS\n) -&gt; fo.Dataset\n</code></pre> <p>compute potential image quality issues such as brightness, contrast, exposure using    the image_quality_issues operator by jacob marks:</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>fiftyone dataset to compute image issues for</p> required <code>operations</code> <code>list(str)</code> <p>list of operations to compute for the dataset</p> <code>IMAGE_ISSUES_OPERATIONS</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>fiftyone dataset with image issues computed</p> Source code in <code>reidhub/assess/utils.py</code> <pre><code>async def fiftyone_check_image_quality_issues(\n    dataset: fo.Dataset,\n    operations: List[str] = IMAGE_ISSUES_OPERATIONS,\n) -&gt; fo.Dataset:\n    \"\"\"\n    compute potential image quality issues such as brightness, contrast, exposure using\\\n    the image_quality_issues operator by jacob marks:\n\n    Args:\n        dataset (fo.Dataset): fiftyone dataset to compute image issues for\n        operations (list(str)): list of operations to compute for the dataset\n\n    Returns:\n        (fo.Dataset): fiftyone dataset with image issues computed\n    \"\"\"\n\n    # Download image issues plugin from github repository\n    fop.download_plugin(\"https://github.com/jacobmarks/image-quality-issues/\")\n\n    for operation in operations:\n        logging.info(f\"Executing {operation} operation\")\n        start_time = time.time()\n        operator = foo.get_operator(f\"@jacobmarks/image_issues/{operation}\")\n        await operator(dataset)\n        end_time = time.time()\n        dataset.save()\n        logging.info(\n            f\"Execution time: {end_time - start_time:.2f} seconds ({operation} operation)\"\n        )\n    return dataset\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":"<p>The Datasets section contains detailed documentation for each re-identification dataset supported in <code>reidhub</code>.  </p> <p>Each dataset page provides:  </p> <p>\ud83d\uddd2\ufe0f High-level information \u2014 quick facts eg number of images &amp; identities, dataset size, species present and conservation status, license, etc.</p> <p>\ud83d\uddbc\ufe0f Sample Image Grid \u2014 a quick visual glimpse of the dataset (especially useful if you\u2019re not familiar with the species).</p> <p>\ud83d\udcd6 Overview \u2014 general background on the dataset.</p> <p>\ud83d\udce5 Access instructions \u2014 how to download and prepare the dataset.</p> <p>\ud83d\uddc2 Standardized metadata \u2014 the unified dataframe schema (metadata_df) used across all datasets.</p> <p>\ud83e\udde9 Enrichments \u2014 additions made to the dataset to make it more usable.</p> <p>\ud83d\udd17 Mirror links \u2014 alternative download mirrors.</p> <p>\ud83d\udd0d Usage notes \u2014 quirks, preprocessing steps, or caveats specific to the dataset.</p> <p>\ud83d\udcca Example workflows \u2014 sample code showing how to integrate the dataset with reidhub.</p> <p>\ud83d\udcd1 Citation \u2014 how to cite the dataset.</p> <p>\ud83d\udcda Publications \u2014 short summaries of related research using the dataset.</p> <p>\ud83d\udca1 Potential use cases \u2014 potential applications of the dataset to be explored.</p>"},{"location":"datasets/#supported-datasets","title":"Supported Datasets","text":"Dataset Species Covered Highlights Great Zebra and Giraffe Count (GZGC) Plains zebra, Masai Giraffe Two days censusing exercise at Nairobi National Park Nyala Data Nyala Field imagery dataset focused on nyala re-identification Amur Tigers Re-identification Amur Tiger Images sampled from videos captured at a conservancy in China <p>Note</p> <p>More datasets upcoming</p>"},{"location":"datasets/#general-workflow","title":"General Workflow","text":"<p>Regardless of dataset, the general usage pattern is the same:  </p> <p>Example with <code>gzgc</code> dataset: <pre><code>from reidhub.access.provenance.gzgc import download_and_extract\ndataset_root = download_and_extract()\nprint(f'Dataset downloaded to: {dataset_root}')\n</code></pre></p> <p>Likewise, for any other dataset already added to <code>reidhub</code>, you can access it by replacing the dataset slug(identifier) ie <code>gzgc</code>. For example, for <code>Nyala Data</code>, you just replace <code>gzgc</code> with <code>nyala</code>:</p> <pre><code>from reidhub.access.provenance.nyala import download_and_extract\ndataset_root = download_and_extract()\nprint(f'Dataset downloaded to: {dataset_root}')\n</code></pre>"},{"location":"datasets/#adding-your-own-dataset","title":"Adding Your Own Dataset","text":"<p><code>reidhub</code> will continue to strive to be extensible. If a dataset you know of isn\u2019t included yet, don't worry, a more user-friendly custom dataset loading functionality will be added soon. </p> <p>Soon, you will be able to add a new dataset to the package hands off using a customized Github Pull Requests template. </p> <p>Watch out for the Contributing guide.  </p> <p>\u2728 Head over to the individual dataset pages in the sidebar to dive into details.  </p>"},{"location":"datasets/gzgc/","title":"Great Zebra and Giraffe Count","text":""},{"location":"datasets/gzgc/#great-zebra-giraffe-count-and-id","title":"\ud83e\udd93\ud83e\udd92 Great Zebra &amp; Giraffe Count and ID","text":""},{"location":"datasets/gzgc/#overview","title":"\ud83d\udcc4 Overview","text":"<p>This dataset contains images taken from a two-day photographic census of Plains zebra and Masai giraffe at Nairobi National Park, Kenya in 2015.   </p> <p>It includes individual IDs for both species, with bounding boxes, metadata such as viewpoint, species, ID; and resized images (\u22643000 px).  </p> <p>The IDs were assigned using the HotSpotter algorithm, Crall et al. 2013 by visually matching the stripes and spots as seen on the body of the animal.</p>"},{"location":"datasets/gzgc/#usage-notes","title":"\u26a0\ufe0f Usage Notes","text":"<ul> <li>Highly imbalanced: many individuals seen only once.  </li> <li>Metadata includes viewpoint; useful for studying orientation effects in ReID.  </li> <li>All images are in the <code>train</code> split; custom validation/test splits needed.  </li> </ul>"},{"location":"datasets/gzgc/#enrichments","title":"\ud83e\udde9 Enrichments","text":""},{"location":"datasets/gzgc/#access","title":"\ud83d\udce5 Access","text":"<p>Original Source: LILA Enriched Version: Hugging Face </p>"},{"location":"datasets/gzgc/#a-provenance","title":"a. Provenance","text":"<p>To access the raw dataset from source, run the following:</p> <pre><code>from reidhub.access.provenance.gzgc import download_and_extract\ndataset_root = download_and_extract()\nprint(f'Dataset downloaded to: {dataset_root}')\n</code></pre>"},{"location":"datasets/gzgc/#b-enriched-re-access","title":"b. Enriched Re-Access","text":"<p>To access the processed, refined and enriched dataset from public mirrors, run the following:</p> <p> </p>"},{"location":"datasets/gzgc/#citation","title":"\ud83d\udcdc Citation","text":"<p>Parham, J., Crall, J., Stewart, C., Berger-Wolf, T., Rubenstein, D.I. Animal population censusing at scale with citizen science and photographic identification. AAAI Spring Symposium, 2017. https://cdn.aaai.org/ocs/15245/15245-68194-1-PB.pdf</p> <pre><code>@inproceedings{parham2017animal,\n  title={Animal population censusing at scale with citizen science and photographic identification},\n  author={Parham, Jason and Crall, Jonathan and Stewart, Charles and Berger-Wolf, Tanya and Rubenstein, Daniel I},\n  booktitle={AAAI spring symposium-technical report},\n  year={2017}\n}\n</code></pre>"},{"location":"datasets/gzgc/#publications","title":"\ud83d\udcda Publications","text":""},{"location":"datasets/gzgc/#potential-usecases","title":"\ud83c\udfaf Potential Usecases","text":"<ul> <li>Re-ID / Retrieval under class imbalance  </li> <li>Few-shot learning: many IDs with 1 image only  </li> <li>Viewpoint analysis for stripes and spot patterns  </li> <li>Combine with other zebra/giraffe datasets for cross-dataset generalization </li> </ul>"},{"location":"getting-started/faq/","title":"FAQ","text":""},{"location":"getting-started/installation/","title":"Installation","text":"<p><code>ReIDHub</code> requires Python 3.9+ and works best inside a virtual environment (poetry is highly recommended).</p> <p>Note</p> <p>The project is still in its early stages of development and is not yet stable. Therefore there is no packaged release as of yet. The recommended way to install is by cloning the repository and installing locally.</p>"},{"location":"getting-started/installation/#installing-from-source","title":"Installing From Source","text":"<p>For now, the best way to install the reidhub package is by cloning the repo from Github and installing it </p> <p>You can install the repo locally using poetry as as shown below: </p> <pre><code># Clone the repository\ngit clone https://github.com/vickruto/reidhub.git\n\n# Move into the project directory\ncd reidhub\n\n# Install the project &amp; its dependencies with Poetry\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#installing-on-colab","title":"Installing on Colab","text":"<p>Since Colab comes with pre-installed packages that may conflict with reidhub dependencies, it\u2019s best to install only the required ones manually.</p> <pre><code># input trunc/full commit hash if you want to load the code as of a particular commit\ncheckpoint_commit_hash = \"\"  \n\ntry:\n    import reidhub \n    print('Loaded pre-installed package')\n\nexcept:\n    import os\n    import sys\n    !git clone https://github.com/vickruto/reidhub.git\n    if checkpoint_commit_hash:\n        !cd reidhub &amp;&amp; git checkout {checkpoint_commit_hash}\n    full_path = os.path.abspath('reidhub')\n    sys.path.insert(0, full_path)\n    print(f'added path {full_path} to sys.path')\n    import reidhub\n    print('\\n\\nSuccessfully cloned and loaded package from github')\n\n!uv pip install fiftyone # install other packages manually\n</code></pre> <p>Here is an example of a Colab compatible notebook: </p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This short guide will help you get up and running with reidhub in just a few steps. It will show you how to access a dataset, standardize its metadata, and inspect the results.</p>"},{"location":"getting-started/quick-start/#1-installation","title":"1. Installation","text":"<p>Make sure you\u2019ve already installed <code>reidhub</code>. If not, check out the Installation guide.</p>"},{"location":"getting-started/quick-start/#2-accessing-a-dataset","title":"2. Accessing a Dataset","text":"<p>With <code>reidhub</code>, you can download and access re-identification datasets in a unified way.</p> <p>Example with <code>gzgc</code> dataset: <pre><code>from reidhub.access.provenance.gzgc import download_and_extract\ndataset_root = download_and_extract()\nprint(f'Dataset downloaded to: {dataset_root}')\n</code></pre></p> <p>These commands will:</p> <ul> <li>Download the Great Zebra and Giraffe Count and ID Dataset dataset (if not already present).  </li> <li>Cache it locally for reuse.  </li> <li>Return a path to the local dataset cache </li> </ul>"},{"location":"getting-started/quick-start/#3-standardizing-metadata","title":"3. Standardizing Metadata","text":"<p>Different datasets often have different formats. <code>reidhub</code> automatically unifies them into a standardized metadata dataframe.</p> <p>Example with <code>gzgc</code> dataset: <pre><code>from reidhub.access.provenance.gzgc import systematize_dataset_metadata\n\n# Convert dataset into a standardized dataframe\nmetadata_df = systematize_dataset_metadata(dataset_root)\n\n# Preview output metadata\nmetadata_df.head()\n</code></pre></p>"},{"location":"getting-started/quick-start/#4-enriching-datasets","title":"4. Enriching Datasets","text":"<p>To make the datasets more usable and make ideating much easier, we generate useful reusable artifacts. For animal re-identification datasets, these include:     1. MegaDetector predictions     2. SAM segmentations     3. Open Clip Embeddings     4. Siglip Embeddings     5. DinoV3 embeddings     5. Spatial feature embeddings eg NVidia C-Radiov3 model embeddings   etc...  </p> <p> </p>"},{"location":"getting-started/quick-start/#5-re-access","title":"5. Re-Access","text":"<p>After initial access of the datasets in the earlier stages and then enriching it, we cache the datasets together with the enrichments and push them to open data repositories including <code>Hugging Face</code>, <code>Kaggle</code> and <code>DagsHub</code> where we can re-access and reuse them. </p> <p> </p>"},{"location":"getting-started/quick-start/#5-whats-next","title":"5. What\u2019s Next?","text":"<ul> <li>Explore the Access Framework to understand provenance and enrichment.  </li> <li>Browse the Supported Datasets page for a full list.  </li> <li>Check out the Example Tutorials for end-to-end workflows.</li> </ul> <p>\ud83d\ude80 That\u2019s it \u2014 you\u2019ve just accessed and standardized your first dataset with <code>reidhub</code>!</p> <p>Note</p> <p>The <code>reidhub</code> project is still in its early stages of development and is actively being developed.  Check back later for new exciting features.  </p>"},{"location":"licences/community-data-license-agreement/","title":"Community data license agreement","text":"<p>The Community Data License Agreement (CDLA) \u2013 Permissive Variant is an open data license designed to facilitate the sharing and use of data while ensuring legal clarity and minimal restrictions. Here's a concise summary:</p> <p>Purpose: It enables data providers to share data openly with the community, allowing others to use, modify, and redistribute the data for any purpose, including commercial uses.</p> <p>Key Features:  - Permissive Use: Users can freely access, use, adapt, and share the data without significant legal barriers, similar to permissive open-source software licenses (e.g., MIT License).  - No Warranty: The data is provided \"as-is,\" with no guarantees about its accuracy, completeness, or fitness for a specific purpose.  - Attribution (Optional): While not always required, the agreement may encourage or require users to credit the data provider when redistributing or using the data.  - No Copyleft: Unlike \"share-alike\" licenses, the CDLA-Permissive does not require derivative datasets or modifications to be shared under the same terms, giving users flexibility.</p> <p>Scope: It applies to data (e.g., datasets, databases) rather than software or creative works, addressing specific legal issues like database rights.</p> <p>Benefits: Promotes collaboration, innovation, and data reuse in fields like research, AI, and analytics by reducing legal uncertainty.</p> <p>This license is ideal for data providers who want to encourage broad use of their data with minimal restrictions. For the full text or specific details, you can refer to the official CDLA website or legal repositories.</p>"},{"location":"pages/acknowledgements/","title":"Acknowledgements","text":"<p>This project was built as a capstone for the Machine Learning Foundations Course (MLFC) by ML@CL @ DSAIL in September 2025. </p> <p>Review the Course Lecture material and Practicals</p> <p>Thanks to Neil, Radzim and Fred for their guidance through the course. </p> <p>This project was greatly inspired by the pods (Python Open Data Science) project by Neil &amp; team.</p> <p>I also learnt a lot from the Wildlife-Tools and Wildlife-Datasets projects while working on this project. </p>"},{"location":"pages/changelog/","title":"Changelog","text":""},{"location":"pages/changelog/#changelog","title":"Changelog","text":"<p>\ud83d\udd01 Evolution of the project over time</p> <p> </p>"},{"location":"pages/contributing/","title":"Contributing","text":"<p>Contribution Guidelines eg:  - adding new datasets to the project.  - fixing errors  </p> <p> </p>"},{"location":"pages/contributing/#adding-a-new-dataset","title":"Adding a New Dataset","text":"<p>Guidelines for adding a new dataset to the <code>reidhub</code> package.</p>"},{"location":"pages/contributing/#fixing-issues","title":"Fixing Issues","text":"<p>Guidelines for fixing issues identified</p>"},{"location":"pages/contributing/#adding-new-features","title":"Adding New Features","text":"<p>Adding a new feature to the datasets/package eg new enrichments etc.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section contains some useful examples for ReIDHub.</p>"},{"location":"tutorials/#1-walkthrough-demo","title":"1. Walkthrough Demo","text":"<p>A walkthrough demo notebook showcasing how to setup, install and use <code>reidhub</code>.</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/","title":"Walkthrough Demo","text":"<p>This work is still in progress and actively being developed. There's probably a newer version of this notebook with new updates. Check it out on Colab  </p> In\u00a0[5]: Copied! <pre>!uv pip install fiftyone\n</pre> !uv pip install fiftyone <pre>Using Python 3.12.11 environment at: /usr\nAudited 1 package in 103ms\n</pre> In\u00a0[6]: Copied! <pre>import os\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n</pre> import os import sys from pathlib import Path import numpy as np import pandas as pd In\u00a0[7]: Copied! <pre>## Load reidhub package\ncheckpoint_commit_hash = \"5deed430e8c9b6ab762c40245808e5da695db1d5\"\n\ntry:\n    import reidhub ## For local development, the package is alread installed using `poetry install`\n    print('Loaded pre-installed package')\n\nexcept:\n    !git clone https://github.com/vickruto/reidhub.git\n    !cd reidhub &amp;&amp; git checkout {checkpoint_commit_hash}\n    full_path = os.path.abspath('reidhub')\n    sys.path.insert(0, full_path)\n    print(full_path)\n    import reidhub\n    print('\\n\\nSuccessfully cloned and loaded package from github')\n</pre> ## Load reidhub package checkpoint_commit_hash = \"5deed430e8c9b6ab762c40245808e5da695db1d5\"  try:     import reidhub ## For local development, the package is alread installed using `poetry install`     print('Loaded pre-installed package')  except:     !git clone https://github.com/vickruto/reidhub.git     !cd reidhub &amp;&amp; git checkout {checkpoint_commit_hash}     full_path = os.path.abspath('reidhub')     sys.path.insert(0, full_path)     print(full_path)     import reidhub     print('\\n\\nSuccessfully cloned and loaded package from github') <pre>Loaded pre-installed package\n</pre> In\u00a0[8]: Copied! <pre>help(reidhub)\n</pre> help(reidhub) <pre>Help on package reidhub:\n\nNAME\n    reidhub\n\nPACKAGE CONTENTS\n    access (package)\n    address (package)\n    assess (package)\n    config\n    tests (package)\n\nFILE\n    /content/reidhub/reidhub/__init__.py\n\n\n</pre> <p>Animal reidentification datasets come in different formats depending on many factors such as:</p> <p>The reidhub package has utilities written to load each of the datasets already added to the package to download them.</p> <p>We are going to demonstrate this with an example dataset that is already added to the package. We will be using the Great Zebra and Giraffe Count Dataset</p> <p>ReIDHub caches datasets that are already downloaded.</p> In\u00a0[9]: Copied! <pre>from reidhub.access.provenance.gzgc import download_and_extract\nhelp(download_and_extract)\n</pre> from reidhub.access.provenance.gzgc import download_and_extract help(download_and_extract) <pre>Help on function download_and_extract in module reidhub.access.provenance.gzgc:\n\ndownload_and_extract() -&gt; str\n    downloads the gzgc from the gcp bucket url provided by lila datasets.\n    Accessible here: https://lila.science/datasets/great-zebra-giraffe-id\n\n    Args:\n        DATASET_ID: str :- the identifier for the dataset.\n\n    returns:\n        a path to the extracted and formatted `reidhub` dataset\n\n</pre> In\u00a0[10]: Copied! <pre>dataset_root = download_and_extract()\nprint(f'Dataset downloaded to: {dataset_root}')\n</pre> dataset_root = download_and_extract() print(f'Dataset downloaded to: {dataset_root}') <pre>Dataset downloaded to: /root/.reidhub_cache/gzgc\n</pre> In\u00a0[11]: Copied! <pre>from reidhub.access.provenance.gzgc import systematize_dataset_metadata\nhelp(systematize_dataset_metadata)\n</pre> from reidhub.access.provenance.gzgc import systematize_dataset_metadata help(systematize_dataset_metadata) <pre>Help on function systematize_dataset_metadata in module reidhub.access.provenance.gzgc:\n\nsystematize_dataset_metadata(dataset_root: str) -&gt; pandas.core.frame.DataFrame\n    Systematize dataset metadata for the Great Zebra and Giraffe Count and ID dataset\n    Args:\n        dataset_root (str) : The root path containing the extracted dataset\n\n    Returns:\n        pd.DataFrame: output dataframe containing systematized metadata\n\n</pre> In\u00a0[12]: Copied! <pre>metadata_df = systematize_dataset_metadata(dataset_root)\n</pre> metadata_df = systematize_dataset_metadata(dataset_root) In\u00a0[13]: Copied! <pre>## Add a full path column\nmetadata_df['fullpath'] = metadata_df['filepath'].apply(\n    lambda x : os.path.join(dataset_root, x)\n)\n</pre> ## Add a full path column metadata_df['fullpath'] = metadata_df['filepath'].apply(     lambda x : os.path.join(dataset_root, x) ) In\u00a0[14]: Copied! <pre>metadata_df\n</pre> metadata_df Out[14]: filepath bbox viewpoint species identity license height width photographer timestamp latitude longitude secondary_identities image_id fullpath 0 gzgc.coco/images/train2020/000000000001.jpg [895.5, 437.0, 1221.0, 690.0] left zebra_plains IBEIS_PZ_1561 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 NNP GZC Car '10WHITE', Person 'A', Image 0005 2015-03-01 14:53:46 -1.351341 36.800374 [2, 1, 3, 3459] 1 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 1 gzgc.coco/images/train2020/000000000002.jpg [951.0, 488.5, 1178.5, 728.5] left zebra_plains IBEIS_PZ_1561 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 NNP GZC Car '10WHITE', Person 'A', Image 0006 2015-03-01 14:53:46 -1.351341 36.800374 [2, 1, 3, 3459] 2 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 2 gzgc.coco/images/train2020/000000000003.jpg [981.0, 552.5, 1131.0, 750.0] left zebra_plains IBEIS_PZ_1561 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 NNP GZC Car '10WHITE', Person 'A', Image 0007 2015-03-01 14:53:52 -1.351341 36.800374 [2, 1, 3, 3459] 3 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 3 gzgc.coco/images/train2020/000000000004.jpg [432.5, 531.0, 1740.0, 938.5] left zebra_plains IBEIS_PZ_1563 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 NNP GZC Car '10WHITE', Person 'A', Image 0008 2015-03-01 14:53:58 -1.351341 36.800374 [4] 4 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 4 gzgc.coco/images/train2020/000000000005.jpg [1568.5, 942.5, 450.0, 462.5] left giraffe_masai NNP_GIRM_0140 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 NNP GZC Car '10WHITE', Person 'A', Image 0010 2015-03-01 15:02:32 -1.367088 36.781978 [6, 7, 5, 2126, 2270] 5 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6920 gzgc.coco/images/train2020/000000004944.jpg [1084.2696629213483, 0.0, 1152.3876404494383, ... left giraffe_masai NNP_GIRM_0074 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 2015-02-26 13:50:34 -1.376729 36.830786 [4724, 6921, 4971] 4944 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 6921 gzgc.coco/images/train2020/000000004945.jpg [779.494382022472, 363.76404494382024, 681.179... left giraffe_masai NNP_GIRM_0030 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 2015-02-26 13:50:40 -1.370916 36.791815 [6923, 6922] 4945 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 6922 gzgc.coco/images/train2020/000000004946.jpg [1358.1460674157304, 0.0, 912.2191011235956, 1... left giraffe_masai NNP_GIRM_0030 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 2015-02-26 13:50:50 -1.370916 36.791815 [6923, 6922] 4946 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 6923 gzgc.coco/images/train2020/000000004947.jpg [1221.2078651685395, 231.03932584269666, 458.5... front giraffe_masai NNP_GIRM_0069 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 2015-02-26 13:51:10 -1.370916 36.791815 [6925, 6924] 4947 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... 6924 gzgc.coco/images/train2020/000000004948.jpg [1127.1067415730338, 37.921348314606746, 689.6... frontleft giraffe_masai NNP_GIRM_0069 http://creativecommons.org/licenses/by-nc-nd/2.0/ 2000 3000 2015-02-26 13:51:17 -1.370916 36.791815 [6925, 6924] 4948 /root/.reidhub_cache/gzgc/gzgc.coco/images/tra... <p>6925 rows \u00d7 15 columns</p> <p>Fiftyone is a powerful tool for interactive visualization of computer vision datasets.</p> <p>Fiftyone's plugin feature allows for extensibility of it's functionality. In the next steps, we are going to be using a few plugins from the Fiftyone community to visualize and enrich our datasets, such as the following:</p> <ul> <li>Image Quality Issues Plugin</li> <li>Dashboard Plugin</li> </ul> In\u00a0[15]: Copied! <pre>from reidhub.access.utils import create_fiftyone_dataset\nhelp(create_fiftyone_dataset)\n</pre> from reidhub.access.utils import create_fiftyone_dataset help(create_fiftyone_dataset) <pre>/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n  return '(?ms)' + res + '\\Z'\n</pre> <pre>Help on function create_fiftyone_dataset in module reidhub.access.utils:\n\ncreate_fiftyone_dataset(root_path: str, metadata_df: pandas.core.frame.DataFrame, dataset_name: str, fields: Optional[List[str]] = None) -&gt; fiftyone.core.dataset.Dataset\n    creates a fiftyone dataset\n    Args:\n        root_path (str) : the root of the dataset containing the images &lt;- also allow pathlib Paths\n        metadata_df (pd.DataFrame) : the metadata for the dataset. Should have the columns (identity, image_path, image_type,)\n        dataset_name (str) : The name of the fiftyone dataset created\n        fields (list of str) : list of fields required in the fiftyone dataset created. should be existing columns in the metadata_df.                 Default : None : Use all the columns in the metadata_df\n    Returns:\n        fo.Dataset: Fiftyone dataset\n\n</pre> In\u00a0[16]: Copied! <pre>dataset = create_fiftyone_dataset(dataset_root, metadata_df, dataset_name='gzgc')\n</pre> dataset = create_fiftyone_dataset(dataset_root, metadata_df, dataset_name='gzgc') <pre>You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n</pre> <pre>WARNING:fiftyone.core.odm.database:You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n</pre> <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6925/6925 [2.4s elapsed, 0s remaining, 3.1K samples/s]      \n</pre> <pre>INFO:eta.core.utils: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6925/6925 [2.4s elapsed, 0s remaining, 3.1K samples/s]      \n</pre> <pre>Computing metadata...\n</pre> <pre>INFO:fiftyone.core.metadata:Computing metadata...\n</pre> <pre> 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6925/6925 [12.4s elapsed, 0s remaining, 538.0 samples/s]      \n</pre> <pre>INFO:eta.core.utils: 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6925/6925 [12.4s elapsed, 0s remaining, 538.0 samples/s]      \n</pre> In\u00a0[17]: Copied! <pre>from reidhub.assess.statics import plot_grid\nhelp(plot_grid)\n</pre> from reidhub.assess.statics import plot_grid help(plot_grid) <pre>Help on function plot_grid in module reidhub.assess.statics:\n\nplot_grid(images: List[Union[numpy.ndarray, PIL.Image.Image]], ids: List[int], grid_shape: Tuple[int, int] = (3, 3), img_size: Tuple[int, int] = (224, 224), spacing: float = 0.05) -&gt; matplotlib.figure.Figure\n    Plot a grid of images with colored borders per identity.\n\n    Args:\n        images (List[Union[np.ndarray, PIL.Image.Image]]): List of images (either numpy arrays or PIL images).\n        ids (List[int]): List of identity labels corresponding to each image.\n        grid_shape (Tuple[int, int], optional): Shape of the grid as (rows, cols). Default is (3, 3).\n        img_size (Tuple[int, int], optional): The (height, width) to resize the images. Default is (224, 224).\n        spacing (float, optional): Fractional spacing between subplots. Default is 0.05.\n\n    Returns:\n        plt.Figure: The figure containing the grid of images with borders.\n\n</pre> In\u00a0[18]: Copied! <pre>## Randomly select images to plot in the grid\nfrom PIL import Image\nn_rows, n_cols = 8, 5 # 5, 8 -- TODO: invert\n\nsamples_df = metadata_df.sample(n_rows*n_cols)\nimages = [Image.open(i) for i in samples_df['fullpath']]\nids = samples_df['identity'].values\ngrid_shape = (n_rows, n_cols)\n</pre> ## Randomly select images to plot in the grid from PIL import Image n_rows, n_cols = 8, 5 # 5, 8 -- TODO: invert  samples_df = metadata_df.sample(n_rows*n_cols) images = [Image.open(i) for i in samples_df['fullpath']] ids = samples_df['identity'].values grid_shape = (n_rows, n_cols) In\u00a0[\u00a0]: Copied! <pre>fig = plot_grid(images, ids, grid_shape)\nfig.savefig('gzgc-grid.png')  # saves the output grid as an image\n</pre> fig = plot_grid(images, ids, grid_shape) fig.savefig('gzgc-grid.png')  # saves the output grid as an image In\u00a0[\u00a0]: Copied! <pre>%%writefile reidhub/reidhub/assess/statics.py\n\"\"\"\nThis module contains functions that are useful for generatic static objects for assessing reid datasets\nExamples: Sample images grid\n          Identity distributions\n          etc\n\n\"\"\"\n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib.cm import get_cmap\nfrom typing import List, Union, Tuple\n\n\ndef plot_grid(\n    images: List[Union[np.ndarray, Image.Image]],\n    ids: List[int],\n    grid_shape: Tuple[int, int] = (3, 3),\n    img_size: Tuple[int, int] = (224, 224),\n    spacing: float = 0.05,\n) -&gt; plt.Figure:\n    \"\"\"\n    Plot a grid of images with colored borders per identity.\n\n    Args:\n        images (List[Union[np.ndarray, PIL.Image.Image]]): List of images (either numpy arrays or PIL images).\n        ids (List[int]): List of identity labels corresponding to each image.\n        grid_shape (Tuple[int, int], optional): Shape of the grid as (rows, cols). Default is (3, 3).\n        img_size (Tuple[int, int], optional): The (height, width) to resize the images. Default is (224, 224).\n        spacing (float, optional): Fractional spacing between subplots. Default is 0.05.\n\n    Returns:\n        plt.Figure: The figure containing the grid of images with borders.\n    \"\"\"\n    # Unpack grid shape\n    cols, rows = grid_shape\n\n    # Calculate total number of cells in the grid\n    n = rows * cols\n\n    # Sample n images if more are provided\n    if len(images) &gt; n:\n        idxs = np.random.choice(len(images), n, replace=False)\n        images = [images[i] for i in idxs]\n        ids = [ids[i] for i in idxs]\n\n    # Normalize identities into a color map\n    unique_ids = sorted(set(ids))\n    cmap = plt.cm.get_cmap(\"tab20\", len(unique_ids))  # Updated cmap access\n    id2color = {uid: cmap(i) for i, uid in enumerate(unique_ids)}\n\n    # Create subplots\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))\n    axes = np.array(axes).reshape(rows, cols)\n\n    # Set transparent background\n    fig.patch.set_alpha(0)\n\n    # Plot each image with its border\n    for ax, img, identity in zip(axes.flatten(), images, ids):\n        # Convert to PIL Image if necessary and resize\n        if not isinstance(img, Image.Image):\n            img = Image.fromarray(img)\n        img_resized = img.resize(img_size)\n\n        # Display image\n        ax.imshow(img_resized)\n        ax.axis(\"off\")\n\n        # Draw border around the image\n        rect = patches.Rectangle(\n            (0, 0),\n            img_size[0],\n            img_size[1],\n            linewidth=10,\n            edgecolor=id2color[identity],\n            facecolor=\"none\",\n            transform=ax.transData,\n        )\n        ax.add_patch(rect)\n\n    # Remove extra axes if fewer images are provided\n    for ax in axes.flatten()[len(images) :]:\n        ax.axis(\"off\")\n\n    # Adjust spacing between subplots\n    plt.subplots_adjust(wspace=spacing, hspace=spacing)\n\n    return fig\n\n\ndef plot_identity_histogram(ids, bins=50, log_scale=False, alpha=0.6, figsize=(8, 5)):\n    \"\"\"\n    Plot a transparent histogram of identity frequencies\n    (how many images per identity).\n\n    Args:\n        ids (list): List of identity labels.\n        bins (int or list): Number of bins or explicit bin edges.\n        log_scale (bool): Whether to use log scale for y-axis.\n        alpha (float): Transparency of histogram bars (0=fully transparent, 1=opaque).\n        figsize (tuple): Figure size.\n    \"\"\"\n    # Count how many images per identity\n    counts = Counter(ids).values()\n\n    # Plot histogram\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.hist(counts, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=alpha)\n\n    # Transparent backgrounds\n    fig.patch.set_alpha(0)   # Figure background\n    ax.patch.set_alpha(0)    # Axes background\n\n    ax.set_xlabel(\"Number of images per identity\")\n    ax.set_ylabel(\"Number of identities\")\n    ax.set_title(\"Identity Frequency Distribution\")\n\n    if log_scale:\n        ax.set_yscale(\"log\")\n        ax.set_ylabel(\"Number of identities (log scale)\")\n\n    plt.tight_layout()\n    return fig\n</pre> %%writefile reidhub/reidhub/assess/statics.py \"\"\" This module contains functions that are useful for generatic static objects for assessing reid datasets Examples: Sample images grid           Identity distributions           etc  \"\"\"  from collections import Counter import matplotlib.pyplot as plt import matplotlib.patches as patches import numpy as np from PIL import Image from matplotlib.cm import get_cmap from typing import List, Union, Tuple   def plot_grid(     images: List[Union[np.ndarray, Image.Image]],     ids: List[int],     grid_shape: Tuple[int, int] = (3, 3),     img_size: Tuple[int, int] = (224, 224),     spacing: float = 0.05, ) -&gt; plt.Figure:     \"\"\"     Plot a grid of images with colored borders per identity.      Args:         images (List[Union[np.ndarray, PIL.Image.Image]]): List of images (either numpy arrays or PIL images).         ids (List[int]): List of identity labels corresponding to each image.         grid_shape (Tuple[int, int], optional): Shape of the grid as (rows, cols). Default is (3, 3).         img_size (Tuple[int, int], optional): The (height, width) to resize the images. Default is (224, 224).         spacing (float, optional): Fractional spacing between subplots. Default is 0.05.      Returns:         plt.Figure: The figure containing the grid of images with borders.     \"\"\"     # Unpack grid shape     cols, rows = grid_shape      # Calculate total number of cells in the grid     n = rows * cols      # Sample n images if more are provided     if len(images) &gt; n:         idxs = np.random.choice(len(images), n, replace=False)         images = [images[i] for i in idxs]         ids = [ids[i] for i in idxs]      # Normalize identities into a color map     unique_ids = sorted(set(ids))     cmap = plt.cm.get_cmap(\"tab20\", len(unique_ids))  # Updated cmap access     id2color = {uid: cmap(i) for i, uid in enumerate(unique_ids)}      # Create subplots     fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))     axes = np.array(axes).reshape(rows, cols)      # Set transparent background     fig.patch.set_alpha(0)      # Plot each image with its border     for ax, img, identity in zip(axes.flatten(), images, ids):         # Convert to PIL Image if necessary and resize         if not isinstance(img, Image.Image):             img = Image.fromarray(img)         img_resized = img.resize(img_size)          # Display image         ax.imshow(img_resized)         ax.axis(\"off\")          # Draw border around the image         rect = patches.Rectangle(             (0, 0),             img_size[0],             img_size[1],             linewidth=10,             edgecolor=id2color[identity],             facecolor=\"none\",             transform=ax.transData,         )         ax.add_patch(rect)      # Remove extra axes if fewer images are provided     for ax in axes.flatten()[len(images) :]:         ax.axis(\"off\")      # Adjust spacing between subplots     plt.subplots_adjust(wspace=spacing, hspace=spacing)      return fig   def plot_identity_histogram(ids, bins=50, log_scale=False, alpha=0.6, figsize=(8, 5)):     \"\"\"     Plot a transparent histogram of identity frequencies     (how many images per identity).      Args:         ids (list): List of identity labels.         bins (int or list): Number of bins or explicit bin edges.         log_scale (bool): Whether to use log scale for y-axis.         alpha (float): Transparency of histogram bars (0=fully transparent, 1=opaque).         figsize (tuple): Figure size.     \"\"\"     # Count how many images per identity     counts = Counter(ids).values()      # Plot histogram     fig, ax = plt.subplots(figsize=figsize)     ax.hist(counts, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=alpha)      # Transparent backgrounds     fig.patch.set_alpha(0)   # Figure background     ax.patch.set_alpha(0)    # Axes background      ax.set_xlabel(\"Number of images per identity\")     ax.set_ylabel(\"Number of identities\")     ax.set_title(\"Identity Frequency Distribution\")      if log_scale:         ax.set_yscale(\"log\")         ax.set_ylabel(\"Number of identities (log scale)\")      plt.tight_layout()     return fig In\u00a0[\u00a0]: Copied! <pre>from reidhub.assess.statics import plot_grid\n</pre> from reidhub.assess.statics import plot_grid In\u00a0[\u00a0]: Copied! <pre>from reidhub.assess.statics import plot_identity_histogram\nhelp(plot_identity_histogram)\n</pre> from reidhub.assess.statics import plot_identity_histogram help(plot_identity_histogram) In\u00a0[\u00a0]: Copied! <pre>from collections import Counter\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib.cm import get_cmap\nfrom typing import List, Union, Tuple\n\ndef plot_identity_histogram(ids, bins=50, log_scale=False, alpha=0.6, figsize=(8, 5)):\n    \"\"\"\n    Plot a transparent histogram of identity frequencies\n    (how many images per identity).\n\n    Args:\n        ids (list): List of identity labels.\n        bins (int or list): Number of bins or explicit bin edges.\n        log_scale (bool): Whether to use log scale for y-axis.\n        alpha (float): Transparency of histogram bars (0=fully transparent, 1=opaque).\n        figsize (tuple): Figure size.\n    \"\"\"\n    # Count how many images per identity\n    counts = Counter(ids).values()\n\n    # Plot histogram\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.hist(counts, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=alpha)\n\n    # Transparent backgrounds\n    fig.patch.set_alpha(0)   # Figure background\n    ax.patch.set_alpha(0)    # Axes background\n\n    ax.set_xlabel(\"Number of images per identity\")\n    ax.set_ylabel(\"Number of identities\")\n    ax.set_title(\"Identity Frequency Distribution\")\n\n    if log_scale:\n        ax.set_yscale(\"log\")\n        ax.set_ylabel(\"Number of identities (log scale)\")\n\n    plt.tight_layout()\n    return fig\n</pre> from collections import Counter import matplotlib.pyplot as plt import matplotlib.patches as patches import numpy as np from PIL import Image from matplotlib.cm import get_cmap from typing import List, Union, Tuple  def plot_identity_histogram(ids, bins=50, log_scale=False, alpha=0.6, figsize=(8, 5)):     \"\"\"     Plot a transparent histogram of identity frequencies     (how many images per identity).      Args:         ids (list): List of identity labels.         bins (int or list): Number of bins or explicit bin edges.         log_scale (bool): Whether to use log scale for y-axis.         alpha (float): Transparency of histogram bars (0=fully transparent, 1=opaque).         figsize (tuple): Figure size.     \"\"\"     # Count how many images per identity     counts = Counter(ids).values()      # Plot histogram     fig, ax = plt.subplots(figsize=figsize)     ax.hist(counts, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=alpha)      # Transparent backgrounds     fig.patch.set_alpha(0)   # Figure background     ax.patch.set_alpha(0)    # Axes background      ax.set_xlabel(\"Number of images per identity\")     ax.set_ylabel(\"Number of identities\")     ax.set_title(\"Identity Frequency Distribution\")      if log_scale:         ax.set_yscale(\"log\")         ax.set_ylabel(\"Number of identities (log scale)\")      plt.tight_layout()     return fig In\u00a0[\u00a0]: Copied! <pre>identities = metadata_df['identity'].values\n\nfig_identities = plot_identity_histogram(identities)\n</pre> identities = metadata_df['identity'].values  fig_identities = plot_identity_histogram(identities) <p>As can be seen from the distributions of identities, most of the animals surveyed during the census were only re-encountered once, making a total of about 1300 animals only seen twice.</p> <p>Some <code>\"celebrity\"</code> animals however sighted more than 50 times. We will be exploring this a bit further using Fiftyone</p> In\u00a0[\u00a0]: Copied! <pre>import fiftyone as fo\n\nsession = fo.launch_app(dataset, auto=False)\n</pre> import fiftyone as fo  session = fo.launch_app(dataset, auto=False) In\u00a0[\u00a0]: Copied! <pre>session.show()\n</pre> session.show() In\u00a0[\u00a0]: Copied! <pre>## Take a snapshot of the current state of the Fiftyone App\nsession.freeze()\n</pre> ## Take a snapshot of the current state of the Fiftyone App session.freeze() In\u00a0[\u00a0]: Copied! <pre>from reidhub.access.utils import fiftyone_check_image_quality_issues\nhelp(fiftyone_check_image_quality_issues)\n</pre> from reidhub.access.utils import fiftyone_check_image_quality_issues help(fiftyone_check_image_quality_issues) In\u00a0[\u00a0]: Copied! <pre># select a subset of operations\n\n# Since these computations are quite computationally heavy, \\\n# we will just compute one of the issues that doesn't require too much compute: aspect ratio\n\nIMAGE_ISSUES_OPERATIONS = [\n    \"compute_aspect_ratio\",\n    # \"compute_brightness\",\n    # \"compute_contrast\",\n    # \"compute_exposure\",\n    # \"compute_saturation\",\n    # \"compute_vignetting\",\n    # \"compute_blurriness\",\n    # \"compute_entropy\",\n]\n</pre> # select a subset of operations  # Since these computations are quite computationally heavy, \\ # we will just compute one of the issues that doesn't require too much compute: aspect ratio  IMAGE_ISSUES_OPERATIONS = [     \"compute_aspect_ratio\",     # \"compute_brightness\",     # \"compute_contrast\",     # \"compute_exposure\",     # \"compute_saturation\",     # \"compute_vignetting\",     # \"compute_blurriness\",     # \"compute_entropy\", ] In\u00a0[\u00a0]: Copied! <pre>dataset = await fiftyone_check_image_quality_issues(dataset, IMAGE_ISSUES_OPERATIONS)\n</pre> dataset = await fiftyone_check_image_quality_issues(dataset, IMAGE_ISSUES_OPERATIONS) <p>We can interactively visualize the image quality issues we have computed using Fiftyone. Some of the things we can :</p> <ol> <li>Filter for images above or below a certain <code>aspect ratio</code> threshold using the slider on the left</li> <li>Sort images by aspect ratio</li> <li>Visualize <code>aspect ratio</code> against other fields such as <code>brightness</code> using the <code>Dashboard</code> plugin</li> </ol> In\u00a0[\u00a0]: Copied! <pre>session.show()\n</pre> session.show() <p>We need to be wary of duplicates since it might lead to data leakage whereby exact or near duplicate images appear in both the training and evaluation subsets of the datasets. To ensure that the models we will be training are not evaluated on images that it has already seen, we need to identify and deal with duplicate images in the dataset.</p> <p>Duplicates occur in camera trap datasets due to a number of reasons:</p> <ol> <li><p>Some datasets are created from sampling video frames. If the camera and the subject do not move over a period of time, then we can get almost the same image from different frames. This can also occur with static images captured in bursts.</p> </li> <li><p>Exact duplicates can occur if the exact same image is renamed and accidently added to the dataset.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># from reidhub.assess.statics import check_for_duplicates\n# help(check_for_duplicates)\n</pre> # from reidhub.assess.statics import check_for_duplicates # help(check_for_duplicates) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#reidhub","title":"ReIDHub\u00b6","text":"<p>Name: Victor Ruto</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#abstract","title":"Abstract\u00b6","text":"<p>Animal re-identification (ReID) is a critical task in wildlife monitoring and conservation research, yet workflows for accessing, processing, and analyzing open-source ReID datasets remain fragmented. This project introduces <code>ReIDHub</code>, a framework structured around the Access\u2013Assess\u2013Address paradigm to streamline dataset-driven research in animal ReID. The framework enables users to: (i) Access open-source ReID datasets in standardized formats and visualize them through tools like FiftyOne; (ii) Assess datasets by computing summary statistics, potential image quality issues such as over exposure or over illumination, and reusable artifacts such as foundation model embeddings and open source model predictions, while caching results; and (iii) Address scientific and practical needs by evaluating the datasets on reidentification benchmark models such as <code>MegaDescriptor</code> and traditional computer vision approaches such as <code>SIFT</code></p> <p>In addition, ReIDHub centralizes dataset-related metadata, including original publications and subsequent research outputs, supported by easily accessible and clear documentation built with MkDocs. By making datasets and analyses more portable, reproducible, and extensible, ReIDHub lowers barriers to entry for animal ReID research and accelerates collaborative conservation science.</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#installs-and-setup","title":"Installs and Setup\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#access","title":"Access\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#loading-a-raw-dataset","title":"Loading a Raw Dataset\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#systematize-the-dataset-metadata","title":"Systematize the Dataset Metadata\u00b6","text":"<p>To be enable standard reproducible animal reidentification workflows using many different datasets, we systematize the dataset metadata. This will ensure that the dataset metadata such as <code>animal identification</code>, <code>viewpoint</code> eg left/right/front etc, <code>species</code>, <code>timestamp</code> and a lot more are standardized across different datasets from different sources.</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#create-fiftyone-dataset","title":"Create Fiftyone Dataset\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#assess","title":"Assess\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#static-visualizations","title":"Static Visualizations\u00b6","text":"<p>We are going to start assessing the downloaded dataset by first plotting a few non-interactive visualizations. Some of the static visualizations implemented in reidhub that we are going to use include:</p> <ul> <li>Sample Image Grids: To showcase some sample images in the dataset. This is just to get a glimpse of the dataset.</li> <li>Identity Distributions: To get a rough idea of the number of sightnings for the animals. Some of the datasets have very long tailed distributions</li> </ul>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#1-show-sample-images","title":"1. Show sample images\u00b6","text":"<p>To get a feel of the dataset and to check that indeed that dataset has been successfully loaded and and preprocessed to systematize the dataset metadata, we are going to plot a grid of sample images from the dataset.</p> <p>The purpose of the static visualizations is just to get a rough idea of what the dataset images look like. Keep in mind that different datasets come in different image formats. Some are already segmented while others are full images. Some of the datasets have different individuals already cropped out from the original images while some come with the full images and include bounding boxes.</p> <p>The images in the grid have different coloured borders to showcase different identities.</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#show-a-grid-of-sample-images","title":"Show a grid of sample images\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#visualize-identity-distributions","title":"Visualize Identity Distributions\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#interactive-visualization-with-fiftyone","title":"Interactive Visualization with Fiftyone\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#issues","title":"Issues:\u00b6","text":"<p>From a visual inspection of the dataset , we can identify a few issues that we will need to deal with:</p> <ol> <li><p>Some of the images are spuriously rotated. The exif data has been stripped from the images and therefore systematically finding these issues will be a problem.</p> </li> <li><p>Some of images are crowded. Each image however, focuses on particular animals. This means we will need to save a version of the dataset where the images are cropped into bounding boxes containing individual animals.</p> </li> <li></li> </ol>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#image-quality-issues","title":"Image quality issues\u00b6","text":"<p>Fiftyone has a powerful plugin utility that allows us to extend it's functionality. Moreover, there a number of plugins contributed by the community that we can use to programmatically identify issues in image issues using simple heuristics such as the average pixel values etc.</p> <p>We will be using the image issues plugin</p>"},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#visualize-image-quality-issues","title":"Visualize Image quality Issues\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#identifying-exact-and-near-duplicates","title":"Identifying Exact and Near Duplicates\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#remove-the-identified-duplicates","title":"Remove The Identified Duplicates\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#save-dataset-to-hugging-face","title":"Save Dataset To Hugging Face\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#data-enrichment","title":"Data Enrichment\u00b6","text":""},{"location":"tutorials/2025-09-22-reidhub-checkpoint/#address","title":"Address\u00b6","text":""}]}